{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "https://www.politifact.com/factchecks/list/?page=1\n",
      "30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Putin bans 5G across Russia, destroys all tow...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video showing celebrities, politicians and oth...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country singer Jason Aldean sued “The View” co...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"WEF chairman Klaus Schwab openly calls for AI...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Douglass Mackey was convicted “because he made...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Charlie Kirk</td>\n",
       "      <td>barely-true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  \\\n",
       "0  “Putin bans 5G across Russia, destroys all tow...   \n",
       "1  Video showing celebrities, politicians and oth...   \n",
       "2  Country singer Jason Aldean sued “The View” co...   \n",
       "3  \"WEF chairman Klaus Schwab openly calls for AI...   \n",
       "4  Douglass Mackey was convicted “because he made...   \n",
       "\n",
       "                                                Link           Date  \\\n",
       "0  https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "1  https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "2  https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "3  https://www.politifact.com/factchecks/2023/aug...  ugust 9, 2023   \n",
       "4  https://www.politifact.com/factchecks/2023/aug...  ugust 9, 2023   \n",
       "\n",
       "            Source        Label  \n",
       "0         Bloggers        false  \n",
       "1   Facebook posts   pants-fire  \n",
       "2   Facebook posts        false  \n",
       "3  Instagram posts        false  \n",
       "4     Charlie Kirk  barely-true  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pagesToGet= 1\n",
    "\n",
    "upperframe=[]  \n",
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
    "    print(url)\n",
    "    \n",
    "    #an exception might be thrown, so the code should be in a try-except block\n",
    "    try:\n",
    "        #use the browser to get the url. This is suspicious command that might blow up.\n",
    "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
    "    \n",
    "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
    "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "        continue                                              #ignore this page. Abandon this and go back.\n",
    "    time.sleep(2)   \n",
    "    soup=BeautifulSoup(page.text,'html.parser')\n",
    "    frame=[]\n",
    "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "    print(len(links))\n",
    "    filename=\"NEWS.csv\"\n",
    "    f=open(filename,\"w\", encoding = 'utf-8')\n",
    "    headers=\"Statement,Link,Date, Source, Label\\n\"\n",
    "    f.write(headers)\n",
    "    \n",
    "    for j in links:\n",
    "        Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
    "        Link = \"https://www.politifact.com\"\n",
    "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
    "        Date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text[-14:-1].strip()\n",
    "        Source = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
    "        Label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
    "        frame.append((Statement,Link,Date,Source,Label))\n",
    "        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\n\")\n",
    "    upperframe.extend(frame)\n",
    "f.close()\n",
    "data=pd.DataFrame(upperframe, columns=['Statement','Link','Date','Source','Label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code for real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building site for  cnn\n",
      "1  Article has date of type None...\n",
      "2  Article has date of type None...\n",
      "3 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/09/us/how-to-help-hawaii-wildfire-victims-iyw/index.html\n",
      "4 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/10/asia/china-jailed-australian-anchor-intl/index.html\n",
      "5 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/sport/live-news/netherlands-spain-japan-sweden-womens-world-cup-quarterfinal/index.html\n",
      "6 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/americas/ecuador-villavicencio-assassination-suspects-colombian-intl-hnk/index.html\n",
      "7 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/travel/viral-icon-of-the-seas-cruise-ship-photo/index.html\n",
      "8 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/style/article/malaysia-swatch-ban-rainbow-lgbt-watch-intl-hnk/index.html\n",
      "9 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/style/rihanna-savage-x-fenty-maternity-line/index.html\n",
      "10 articles downloaded from cnn  using newspaper, url:  https://edition.cnn.com/us/live-news/hawaii-maui-wildfires-08-11-23/index.html\n",
      "11 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/us/live-news/hawaii-maui-wildfires-08-11-23/index.html\n",
      "12 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/us/maui-wildfires-hurricane-dora-friday/index.html\n",
      "13 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/10/us/factors-fueling-maui-fires-climate/index.html\n",
      "14 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/10/us/satellite-images-maui-fires-damage/index.html\n",
      "15 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/sport/collin-morikawa-birdies-hawaii-wildfire-donation-spt-intl/index.html\n",
      "16 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/europe/live-news/russia-ukraine-war-new-08-11-23/index.html\n",
      "17 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/10/europe/poland-troops-belarus-border-intl/index.html\n",
      "18 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/china/china-national-security-spy-cia-intl-hnk/index.html\n",
      "19 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/investing/china-country-garden-loss-warning-intl-hnk/index.html\n",
      "20 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/india/india-rajinikanth-jailer-movie-release-intl-hnk/index.html\n",
      "21 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/africa/zuma-south-africa-prison-release-intl/index.html\n",
      "22 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/politics/trump-trial-date-2024-campaign/index.html\n",
      "23 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/politics/trump-protective-order-chutkan-hearing/index.html\n",
      "24 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/economy/uk-economy-gdp-q2/index.html\n",
      "25 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/health/doxypep-sti-prevention-pill/index.html\n",
      "26 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/business/subway-india-cheese-slices-inflation/index.html\n",
      "27 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/10/golf/phil-mickelson-bet-ryder-cup/index.html\n",
      "28 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/2023/08/11/energy/oil-prices-2023-iea/index.html\n",
      "29 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/travel/american-family-open-restaurant-in-italy/index.html\n",
      "30 articles downloaded from cnn  using newspaper, url:  http://edition.cnn.com/style/look-of-the-week-alana-hadid-copenhagen/index.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     content\u001b[39m.\u001b[39;49mdownload()\n\u001b[1;32m    111\u001b[0m     content\u001b[39m.\u001b[39mparse()\n\u001b[1;32m    112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/newspaper/article.py:170\u001b[0m, in \u001b[0;36mArticle.download\u001b[0;34m(self, input_html, title, recursion_counter)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m input_html \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         html \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mget_html_2XX_only(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n\u001b[1;32m    171\u001b[0m     \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_state \u001b[39m=\u001b[39m ArticleDownloadState\u001b[39m.\u001b[39mFAILED_RESPONSE\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/newspaper/network.py:62\u001b[0m, in \u001b[0;36mget_html_2XX_only\u001b[0;34m(url, config, response)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_html_from_response(response)\n\u001b[0;32m---> 62\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m     63\u001b[0m     url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mget_request_kwargs(timeout, useragent, proxies, headers))\n\u001b[1;32m     65\u001b[0m html \u001b[39m=\u001b[39m _get_html_from_response(response)\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mhttp_success_only:\n\u001b[1;32m     68\u001b[0m     \u001b[39m# fail if HTTP sends a non 2XX response\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dictionary = {\n",
    "  \"cnn\": {\n",
    "    \"link\": \"http://edition.cnn.com/\"\n",
    "  },\n",
    "  \"bbc\": {\n",
    "    \"rss\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
    "    \"link\": \"http://www.bbc.com/\"\n",
    "  },\n",
    "  \"theguardian\": {\n",
    "    \"rss\": \"https://www.theguardian.com/uk/rss\",\n",
    "    \"link\": \"https://www.theguardian.com/international\"\n",
    "  },\n",
    "  \"breitbart\": {\n",
    "    \"link\": \"http://www.breitbart.com/\"\n",
    "  },\n",
    "  \"infowars\": {\n",
    "    \"link\": \"https://www.infowars.com/\"\n",
    "  },\n",
    "  \"foxnews\": {\n",
    "    \"link\": \"http://www.foxnews.com/\"\n",
    "  },\n",
    "  \"nbcnews\": {\n",
    "    \"link\": \"http://www.nbcnews.com/\"\n",
    "  },\n",
    "  \"washingtonpost\": {\n",
    "    \"rss\": \"http://feeds.washingtonpost.com/rss/world\",\n",
    "    \"link\": \"https://www.washingtonpost.com/\"\n",
    "  },\n",
    "  \"theonion\": {\n",
    "    \"link\": \"http://www.theonion.com/\"\n",
    "  }\n",
    "}\n",
    "\n",
    "json_object = json.dumps(dictionary, indent = 4) \n",
    "with open(\"NewsPapers.json\", \"w\") as outfile: \n",
    "    outfile.write(json_object) \n",
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the limit for number of articles to download\n",
    "LIMIT = 14500\n",
    "\n",
    "data = {}\n",
    "data['newspapers'] = {}\n",
    "\n",
    "# Loads the JSON files with news sites\n",
    "with open('NewsPapers.json') as data_file:\n",
    "    companies = json.load(data_file)\n",
    "\n",
    "count = 1\n",
    "\n",
    "# Iterate through each news company\n",
    "for company, value in companies.items():\n",
    "    # If a RSS link is provided in the JSON file, this will be the first choice.\n",
    "    # Reason for this is that, RSS feeds often give more consistent and correct data.\n",
    "    # If you do not want to scrape from the RSS-feed, just leave the RSS attr empty in the JSON file.\n",
    "    if 'rss' in value:\n",
    "        d = fp.parse(value['rss'])\n",
    "        print(\"Downloading articles from \", company)\n",
    "        newsPaper = {\n",
    "            \"rss\": value['rss'],\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        for entry in d.entries:\n",
    "            # Check if publish date is provided, if no the article is skipped.\n",
    "            # This is done to keep consistency in the data and to keep the script from crashing.\n",
    "            if hasattr(entry, 'published'):\n",
    "                if count > LIMIT:\n",
    "                    break\n",
    "                article = {}\n",
    "                article['link'] = entry.link\n",
    "                date = entry.published_parsed\n",
    "                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n",
    "                try:\n",
    "                    content = Article(entry.link)\n",
    "                    content.download()\n",
    "                    content.parse()\n",
    "                except Exception as e:\n",
    "                    # If the download for some reason fails (ex. 404) the script will continue downloading\n",
    "                    # the next article.\n",
    "                    print(e)\n",
    "                    print(\"continuing...\")\n",
    "                    continue\n",
    "                article['title'] = content.title\n",
    "                article['text'] = content.text\n",
    "                newsPaper['articles'].append(article)\n",
    "                print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n",
    "                count = count + 1\n",
    "    else:\n",
    "        # This is the fallback method if a RSS-feed link is not provided.\n",
    "        # It uses the python newspaper library to extract articles\n",
    "        print(\"Building site for \", company)\n",
    "        paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "        newsPaper = {\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        noneTypeCount = 0\n",
    "        for content in paper.articles:\n",
    "            if count > LIMIT:\n",
    "                break\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "                continue\n",
    "            # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "            if content.publish_date is None:\n",
    "                print(count, \" Article has date of type None...\")\n",
    "                noneTypeCount = noneTypeCount + 1\n",
    "                if noneTypeCount > 100:\n",
    "                    print(\"Too many noneType dates, aborting...\")\n",
    "                    noneTypeCount = 0\n",
    "                    break\n",
    "                count = count + 1\n",
    "                continue\n",
    "            article = {}\n",
    "            article['title'] = content.title\n",
    "            article['text'] = content.text\n",
    "            article['link'] = content.url\n",
    "            article['published'] = content.publish_date.isoformat()\n",
    "            newsPaper['articles'].append(article)\n",
    "            print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n",
    "            count = count + 1\n",
    "            noneTypeCount = 0\n",
    "    count = 1\n",
    "    data['newspapers'][company] = newsPaper\n",
    "\n",
    "# Finally it saves the articles as a JSON-file.\n",
    "try:\n",
    "    with open('scraped_articles.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "except Exception as e: print(e)\n",
    "with open('scraped_articles.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "for i, site in enumerate((list(d['newspapers']))):\n",
    "    print(i, site)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "for i, site in enumerate((list(d['newspapers']))):\n",
    "    articles = list(d['newspapers'][site]['articles'])\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame.from_dict(articles)\n",
    "        df[\"site\"] = site\n",
    "    else:\n",
    "        new_df = pd.DataFrame.from_dict(articles)\n",
    "        new_df[\"site\"] = site\n",
    "        df = pd.concat([df, new_df], ignore_index = True)     \n",
    "df.shape\n",
    "(1844, 5)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping code modifucation trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles from cnn\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/11/economy/uk-economy-gdp-q2/index.html on URL https://money.cnn.com/2023/08/11/economy/uk-economy-gdp-q2/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/11/business/subway-india-cheese-slices-inflation/index.html on URL https://money.cnn.com/2023/08/11/business/subway-india-cheese-slices-inflation/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/11/investing/china-country-garden-loss-warning-intl-hnk/index.html on URL https://money.cnn.com/2023/08/11/investing/china-country-garden-loss-warning-intl-hnk/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/media/writers-strike-negotiations-meeting/index.html on URL https://money.cnn.com/2023/08/10/media/writers-strike-negotiations-meeting/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/media/linda-yaccarino-elon-musk-reliable-sources/index.html on URL https://money.cnn.com/2023/08/10/media/linda-yaccarino-elon-musk-reliable-sources/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/business/new-zealand-gender-pay-gap-intl-hnk/index.html on URL https://money.cnn.com/2023/08/10/business/new-zealand-gender-pay-gap-intl-hnk/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/business/mushrooms-abu-dhabi-desert-below-farm-spc/index.html on URL https://money.cnn.com/2023/08/10/business/mushrooms-abu-dhabi-desert-below-farm-spc/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/business/charity-real-donation-money-going/index.html on URL https://money.cnn.com/2023/08/10/business/charity-real-donation-money-going/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/tech/biden-administration-social-media-contacts/index.html on URL https://money.cnn.com/2023/08/10/tech/biden-administration-social-media-contacts/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/tech/portable-cell-service-maui-outages/index.html on URL https://money.cnn.com/2023/08/10/tech/portable-cell-service-maui-outages/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/tech/x-ceo-linda-yaccarino-threads-meta/index.html on URL https://money.cnn.com/2023/08/10/tech/x-ceo-linda-yaccarino-threads-meta/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/09/tech/cell-service-outages-maui-fires/index.html on URL https://money.cnn.com/2023/08/09/tech/cell-service-outages-maui-fires/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/homes/mortgage-rates-august-10/index.html on URL https://money.cnn.com/2023/08/10/homes/mortgage-rates-august-10/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/tech/arkady-volozh-yandex-ukraine-war/index.html on URL https://money.cnn.com/2023/08/10/tech/arkady-volozh-yandex-ukraine-war/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/business/grocery-prices-meat-july/index.html on URL https://money.cnn.com/2023/08/10/business/grocery-prices-meat-july/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/business/barbie-pink-retail-boom/index.html on URL https://money.cnn.com/2023/08/10/business/barbie-pink-retail-boom/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/economy/maui-economy-tourism-wildfires/index.html on URL https://money.cnn.com/2023/08/10/economy/maui-economy-tourism-wildfires/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/economy/cpi-inflation-july/index.html on URL https://money.cnn.com/2023/08/10/economy/cpi-inflation-july/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/media/disney-plus-streaming-prices-reliable-sources/index.html on URL https://money.cnn.com/2023/08/10/media/disney-plus-streaming-prices-reliable-sources/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/tech/twitter-auction-items/index.html on URL https://money.cnn.com/2023/08/10/tech/twitter-auction-items/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/economy/credit-card-car-loan-pay-failure-pre-covid/index.html on URL https://money.cnn.com/2023/08/10/economy/credit-card-car-loan-pay-failure-pre-covid/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/homes/forecast-shelter-inflation/index.html on URL https://money.cnn.com/2023/08/10/homes/forecast-shelter-inflation/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/tech/ai-generated-books-amazon/index.html on URL https://money.cnn.com/2023/08/10/tech/ai-generated-books-amazon/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/05/business/movie-theater-box-office-air-conditioning/index.html on URL https://money.cnn.com/2023/08/05/business/movie-theater-box-office-air-conditioning/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/04/business/gas-prices-extreme-heat/index.html on URL https://money.cnn.com/2023/08/04/business/gas-prices-extreme-heat/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/04/business/japan-calbee-potato-shortage-climate-intl-hnk/index.html on URL https://money.cnn.com/2023/08/04/business/japan-calbee-potato-shortage-climate-intl-hnk/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/03/business/india-rice-export-ban/index.html on URL https://money.cnn.com/2023/08/03/business/india-rice-export-ban/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/08/business/penn-entertainment-barstool-sports-espn/index.html on URL https://money.cnn.com/2023/08/08/business/penn-entertainment-barstool-sports-espn/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/07/media/hollywood-studios-writers-strike-reliable-sources/index.html on URL https://money.cnn.com/2023/08/07/media/hollywood-studios-writers-strike-reliable-sources/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/03/media/warner-bros-discovery-earnings/index.html on URL https://money.cnn.com/2023/08/03/media/warner-bros-discovery-earnings/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/07/28/media/india-bollywood-bawaal-movie-holocaust-controversy-intl-hnk/index.html on URL https://money.cnn.com/2023/07/28/media/india-bollywood-bawaal-movie-holocaust-controversy-intl-hnk/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/07/26/success/interest-rates-consumers-federal-reserve/index.html on URL https://money.cnn.com/2023/07/26/success/interest-rates-consumers-federal-reserve/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/07/17/success/menopause-at-work/index.html on URL https://money.cnn.com/2023/07/17/success/menopause-at-work/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/07/06/success/how-much-to-feel-rich/index.html on URL https://money.cnn.com/2023/07/06/success/how-much-to-feel-rich/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/06/26/success/hot-desking-hoteling-dynamic-seating-hybrid/index.html on URL https://money.cnn.com/2023/06/26/success/hot-desking-hoteling-dynamic-seating-hybrid/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2023/08/10/media/lebanon-barbie-ban-homosexuality/index.html on URL https://money.cnn.com/2023/08/10/media/lebanon-barbie-ban-homosexuality/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/cnn-underscored/travel/book-travel-activities-with-chase-points on URL https://money.cnn.com/cnn-underscored/travel/book-travel-activities-with-chase-points continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/cnn-underscored/money/aeroplan-air-canada-credit-card-bonus on URL https://money.cnn.com/cnn-underscored/money/aeroplan-air-canada-credit-card-bonus continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/cnn-underscored/money/best-credit-card-elite-status-benefits on URL https://money.cnn.com/cnn-underscored/money/best-credit-card-elite-status-benefits continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/04/27/business/diamonds-manmade-demand/index.html on URL https://money.cnn.com/2022/04/27/business/diamonds-manmade-demand/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/04/25/business/electric-hybrid-corvette/index.html on URL https://money.cnn.com/2022/04/25/business/electric-hybrid-corvette/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/04/21/business/bereal-social-media-app-cec/index.html on URL https://money.cnn.com/2022/04/21/business/bereal-social-media-app-cec/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/04/20/business/bmw-i7/index.html on URL https://money.cnn.com/2022/04/20/business/bmw-i7/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/travel/article/plane-airbus-cold-weather-testing/index.html on URL https://money.cnn.com/travel/article/plane-airbus-cold-weather-testing/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/11/11/business/sbf-wealth/index.html on URL https://money.cnn.com/2022/11/11/business/sbf-wealth/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/11/10/business/ftx-sam-bankman-fried-crypto-collapse/index.html on URL https://money.cnn.com/2022/11/10/business/ftx-sam-bankman-fried-crypto-collapse/index.html continuing...\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://money.cnn.com/2022/10/25/tech/amazon-labor-union-california-withdraw-petition/index.html on URL https://money.cnn.com/2022/10/25/tech/amazon-labor-union-california-withdraw-petition/index.html continuing...\n",
      "Scraping articles from bbc\n",
      "Scraping articles from theguardian\n",
      "Scraping articles from breitbart\n",
      "Scraping articles from infowars\n",
      "Scraping articles from foxnews\n",
      "Scraping articles from nbcnews\n",
      "Scraping articles from washingtonpost\n",
      "Scraping articles from theonion\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary of news sites\n",
    "sites = {\n",
    "    \"cnn\": {\n",
    "        \"link\": \"http://edition.cnn.com/\"\n",
    "    },\n",
    "    \"bbc\": {\n",
    "        \"rss\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
    "        \"link\": \"http://www.bbc.com/\"\n",
    "    },\n",
    "    \"theguardian\": {\n",
    "        \"rss\": \"https://www.theguardian.com/uk/rss\",\n",
    "        \"link\": \"https://www.theguardian.com/international\"\n",
    "    },\n",
    "    \"breitbart\": {\n",
    "        \"link\": \"http://www.breitbart.com/\"\n",
    "    },\n",
    "    \"infowars\": {\n",
    "        \"link\": \"https://www.infowars.com/\"\n",
    "    },\n",
    "    \"foxnews\": {\n",
    "        \"link\": \"http://www.foxnews.com/\"\n",
    "    },\n",
    "    \"nbcnews\": {\n",
    "        \"link\": \"http://www.nbcnews.com/\"\n",
    "    },\n",
    "    \"washingtonpost\": {\n",
    "        \"rss\": \"http://feeds.washingtonpost.com/rss/world\",\n",
    "        \"link\": \"https://www.washingtonpost.com/\"\n",
    "    },\n",
    "    \"theonion\": {\n",
    "        \"link\": \"http://www.theonion.com/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "LIMIT = 100\n",
    "data = []\n",
    "\n",
    "# Iterate through each news company\n",
    "for site, info in sites.items():\n",
    "    print(\"Scraping articles from\", site)\n",
    "\n",
    "    if 'rss' in info:\n",
    "        d = fp.parse(info['rss'])\n",
    "        for entry in d.entries:\n",
    "            if len(data) >= LIMIT:\n",
    "                break\n",
    "            if hasattr(entry, 'published'):\n",
    "                article = {\n",
    "                    'site': site,\n",
    "                    'link': entry.link,\n",
    "                    'published': datetime.fromtimestamp(mktime(entry.published_parsed)).isoformat()\n",
    "                }\n",
    "                try:\n",
    "                    content = Article(entry.link)\n",
    "                    content.download()\n",
    "                    content.parse()\n",
    "                    article['title'] = content.title\n",
    "                    article['text'] = content.text\n",
    "                    data.append(article)\n",
    "                except Exception as e:\n",
    "                    print(e, \"continuing...\")\n",
    "                    continue\n",
    "    else:\n",
    "        paper = newspaper.build(info['link'], memoize_articles=False)\n",
    "        for content in paper.articles:\n",
    "            if len(data) >= LIMIT:\n",
    "                break\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                print(e, \"continuing...\")\n",
    "                continue\n",
    "            if content.publish_date is None:\n",
    "                continue\n",
    "            article = {\n",
    "                'site': site,\n",
    "                'title': content.title,\n",
    "                'text': content.text,\n",
    "                'link': content.url,\n",
    "                'published': content.publish_date.isoformat()\n",
    "            }\n",
    "            data.append(article)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print DataFrame shape\n",
    "print(df.shape)\n",
    "# Save DataFrame as CSV\n",
    "df.to_csv('news_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page: 1\n",
      "processing page: 2\n",
      "processing page: 3\n",
      "processing page: 4\n",
      "processing page: 5\n",
      "processing page: 6\n",
      "processing page: 7\n",
      "processing page: 8\n",
      "processing page: 9\n",
      "processing page: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Putin bans 5G across Russia, destroys all tow...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video showing celebrities, politicians and oth...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country singer Jason Aldean sued “The View” co...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"WEF chairman Klaus Schwab openly calls for AI...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Douglass Mackey was convicted “because he made...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Charlie Kirk</td>\n",
       "      <td>barely-true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  \\\n",
       "0  “Putin bans 5G across Russia, destroys all tow...   \n",
       "1  Video showing celebrities, politicians and oth...   \n",
       "2  Country singer Jason Aldean sued “The View” co...   \n",
       "3  \"WEF chairman Klaus Schwab openly calls for AI...   \n",
       "4  Douglass Mackey was convicted “because he made...   \n",
       "\n",
       "                                                Link           Date  \\\n",
       "0  https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "1  https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "2  https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "3  https://www.politifact.com/factchecks/2023/aug...  ugust 9, 2023   \n",
       "4  https://www.politifact.com/factchecks/2023/aug...  ugust 9, 2023   \n",
       "\n",
       "            Source        Label  \n",
       "0         Bloggers        false  \n",
       "1   Facebook posts   pants-fire  \n",
       "2   Facebook posts        false  \n",
       "3  Instagram posts        false  \n",
       "4     Charlie Kirk  barely-true  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "articlesToScrape = 500\n",
    "\n",
    "upperframe = []\n",
    "\n",
    "for page in range(1, (articlesToScrape // 50) + 1):\n",
    "    print('processing page:', page)\n",
    "    url = 'https://www.politifact.com/factchecks/list/?page=' + str(page)\n",
    "\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "    except Exception as e:\n",
    "        error_type, error_obj, error_info = sys.exc_info()\n",
    "        print('ERROR FOR LINK:', url)\n",
    "        print(error_type, 'Line:', error_info.tb_lineno)\n",
    "        continue\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    frame = []\n",
    "    links = soup.find_all('li', attrs={'class': 'o-listicle__item'})\n",
    "    \n",
    "    for j in links:\n",
    "        Statement = j.find(\"div\", attrs={'class': 'm-statement__quote'}).text.strip()\n",
    "        Link = \"https://www.politifact.com\" + j.find(\"div\", attrs={'class': 'm-statement__quote'}).find('a')['href'].strip()\n",
    "        Date = j.find('div', attrs={'class': 'm-statement__body'}).find('footer').text[-14:-1].strip()\n",
    "        Source = j.find('div', attrs={'class': 'm-statement__meta'}).find('a').text.strip()\n",
    "        Label = j.find('div', attrs={'class': 'm-statement__content'}).find('img', attrs={'class': 'c-image__original'}).get('alt').strip()\n",
    "        frame.append((Statement, Link, Date, Source, Label))\n",
    "\n",
    "    upperframe.extend(frame)\n",
    "\n",
    "    if len(upperframe) >= articlesToScrape:\n",
    "        break\n",
    "\n",
    "data = pd.DataFrame(upperframe, columns=['Statement', 'Link', 'Date', 'Source', 'Label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Putin bans 5G across Russia, destroys all tow...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video showing celebrities, politicians and oth...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country singer Jason Aldean sued “The View” co...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"WEF chairman Klaus Schwab openly calls for AI...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Douglass Mackey was convicted “because he made...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/aug...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Charlie Kirk</td>\n",
       "      <td>barely-true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Jamie Foxx tras una coagulación de sangre en e...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/jun...</td>\n",
       "      <td>June 7, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>“New tapes” of Nancy Pelosi show that the Jan....</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/jun...</td>\n",
       "      <td>June 6, 2023</td>\n",
       "      <td>Benny Johnson</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Photos show Target selling children’s clothing...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/jun...</td>\n",
       "      <td>June 6, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Disney \"acaba de publicar\" que no puedes entra...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/jun...</td>\n",
       "      <td>June 6, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Texas Attorney General Ken Paxton was “caught ...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2023/jun...</td>\n",
       "      <td>June 6, 2023</td>\n",
       "      <td>TikTok posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Statement  \\\n",
       "0    “Putin bans 5G across Russia, destroys all tow...   \n",
       "1    Video showing celebrities, politicians and oth...   \n",
       "2    Country singer Jason Aldean sued “The View” co...   \n",
       "3    \"WEF chairman Klaus Schwab openly calls for AI...   \n",
       "4    Douglass Mackey was convicted “because he made...   \n",
       "..                                                 ...   \n",
       "295  Jamie Foxx tras una coagulación de sangre en e...   \n",
       "296  “New tapes” of Nancy Pelosi show that the Jan....   \n",
       "297  Photos show Target selling children’s clothing...   \n",
       "298  Disney \"acaba de publicar\" que no puedes entra...   \n",
       "299  Texas Attorney General Ken Paxton was “caught ...   \n",
       "\n",
       "                                                  Link           Date  \\\n",
       "0    https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "1    https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "2    https://www.politifact.com/factchecks/2023/aug...  gust 10, 2023   \n",
       "3    https://www.politifact.com/factchecks/2023/aug...  ugust 9, 2023   \n",
       "4    https://www.politifact.com/factchecks/2023/aug...  ugust 9, 2023   \n",
       "..                                                 ...            ...   \n",
       "295  https://www.politifact.com/factchecks/2023/jun...   June 7, 2023   \n",
       "296  https://www.politifact.com/factchecks/2023/jun...   June 6, 2023   \n",
       "297  https://www.politifact.com/factchecks/2023/jun...   June 6, 2023   \n",
       "298  https://www.politifact.com/factchecks/2023/jun...   June 6, 2023   \n",
       "299  https://www.politifact.com/factchecks/2023/jun...   June 6, 2023   \n",
       "\n",
       "              Source        Label  \n",
       "0           Bloggers        false  \n",
       "1     Facebook posts   pants-fire  \n",
       "2     Facebook posts        false  \n",
       "3    Instagram posts        false  \n",
       "4       Charlie Kirk  barely-true  \n",
       "..               ...          ...  \n",
       "295   Facebook posts        false  \n",
       "296    Benny Johnson        false  \n",
       "297   Facebook posts        false  \n",
       "298   Facebook posts        false  \n",
       "299     TikTok posts        false  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page: 1\n",
      "Processing page: 2\n",
      "Processing page: 3\n",
      "Processing page: 4\n",
      "                                           Statement  \\\n",
      "0  “Putin bans 5G across Russia, destroys all tow...   \n",
      "1  Video showing celebrities, politicians and oth...   \n",
      "2  Country singer Jason Aldean sued “The View” co...   \n",
      "3  \"WEF chairman Klaus Schwab openly calls for AI...   \n",
      "4  Douglass Mackey was convicted “because he made...   \n",
      "\n",
      "                                                Link           Date  \\\n",
      "0  https://www.politifact.com/personalities/blog-...  gust 10, 2023   \n",
      "1  https://www.politifact.com/personalities/faceb...  gust 10, 2023   \n",
      "2  https://www.politifact.com/personalities/faceb...  gust 10, 2023   \n",
      "3  https://www.politifact.com/personalities/insta...  ugust 9, 2023   \n",
      "4  https://www.politifact.com/personalities/charl...  ugust 9, 2023   \n",
      "\n",
      "            Source  Label  \n",
      "0         Bloggers  false  \n",
      "1   Facebook posts  false  \n",
      "2   Facebook posts  false  \n",
      "3  Instagram posts  false  \n",
      "4     Charlie Kirk  false  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set the number of articles to scrape\n",
    "articles_to_scrape = 200\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through pages until desired number of articles is reached\n",
    "for page in range(1, (articles_to_scrape // 50) + 1):\n",
    "    print('Processing page:', page)\n",
    "    \n",
    "    # Construct the URL for the page\n",
    "    url = f'https://www.politifact.com/factchecks/list/?page={page}'\n",
    "\n",
    "    try:\n",
    "        # Get the page content\n",
    "        page = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print('Error fetching URL:', url)\n",
    "        continue\n",
    "    \n",
    "    # Pause for a moment to avoid overwhelming the server\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Parse the page content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Find all the list items containing article information\n",
    "    articles = soup.find_all('li', class_='o-listicle__item')\n",
    "    \n",
    "    # Loop through each article\n",
    "    for article in articles:\n",
    "        # Extract information from the article\n",
    "        statement = article.find('div', class_='m-statement__quote').text.strip()\n",
    "        link = \"https://www.politifact.com\" + article.find('a')['href'].strip()\n",
    "        date = article.find('footer').text[-14:-1].strip()\n",
    "        source = article.find('div', class_='m-statement__meta').find('a').text.strip()\n",
    "        label = j.find('div', attrs={'class': 'm-statement__content'}).find('img', attrs={'class': 'c-image__original'}).get('alt').strip()\n",
    "        \n",
    "        # Append the article information to the data list\n",
    "        data.append((statement, link, date, source, label))\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data, columns=['Statement', 'Link', 'Date', 'Source', 'Label'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Putin bans 5G across Russia, destroys all tow...</td>\n",
       "      <td>https://www.politifact.com/personalities/blog-...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video showing celebrities, politicians and oth...</td>\n",
       "      <td>https://www.politifact.com/personalities/faceb...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Country singer Jason Aldean sued “The View” co...</td>\n",
       "      <td>https://www.politifact.com/personalities/faceb...</td>\n",
       "      <td>gust 10, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"WEF chairman Klaus Schwab openly calls for AI...</td>\n",
       "      <td>https://www.politifact.com/personalities/insta...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Douglass Mackey was convicted “because he made...</td>\n",
       "      <td>https://www.politifact.com/personalities/charl...</td>\n",
       "      <td>ugust 9, 2023</td>\n",
       "      <td>Charlie Kirk</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>“Miss USA boycotts Miss Universe pageant: ‘I’m...</td>\n",
       "      <td>https://www.politifact.com/personalities/faceb...</td>\n",
       "      <td>July 17, 2023</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>AMC is intentionally deterring people from see...</td>\n",
       "      <td>https://www.politifact.com/personalities/tikto...</td>\n",
       "      <td>July 14, 2023</td>\n",
       "      <td>TikTok posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>“Joe Biden announced that the U.S. dollar will...</td>\n",
       "      <td>https://www.politifact.com/personalities/insta...</td>\n",
       "      <td>July 14, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>“No human is allowed to travel to Antarctica.”</td>\n",
       "      <td>https://www.politifact.com/personalities/insta...</td>\n",
       "      <td>July 14, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>“Donald Trump arrested” July 5.</td>\n",
       "      <td>https://www.politifact.com/personalities/insta...</td>\n",
       "      <td>July 14, 2023</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Statement  \\\n",
       "0    “Putin bans 5G across Russia, destroys all tow...   \n",
       "1    Video showing celebrities, politicians and oth...   \n",
       "2    Country singer Jason Aldean sued “The View” co...   \n",
       "3    \"WEF chairman Klaus Schwab openly calls for AI...   \n",
       "4    Douglass Mackey was convicted “because he made...   \n",
       "..                                                 ...   \n",
       "115  “Miss USA boycotts Miss Universe pageant: ‘I’m...   \n",
       "116  AMC is intentionally deterring people from see...   \n",
       "117  “Joe Biden announced that the U.S. dollar will...   \n",
       "118     “No human is allowed to travel to Antarctica.”   \n",
       "119                    “Donald Trump arrested” July 5.   \n",
       "\n",
       "                                                  Link           Date  \\\n",
       "0    https://www.politifact.com/personalities/blog-...  gust 10, 2023   \n",
       "1    https://www.politifact.com/personalities/faceb...  gust 10, 2023   \n",
       "2    https://www.politifact.com/personalities/faceb...  gust 10, 2023   \n",
       "3    https://www.politifact.com/personalities/insta...  ugust 9, 2023   \n",
       "4    https://www.politifact.com/personalities/charl...  ugust 9, 2023   \n",
       "..                                                 ...            ...   \n",
       "115  https://www.politifact.com/personalities/faceb...  July 17, 2023   \n",
       "116  https://www.politifact.com/personalities/tikto...  July 14, 2023   \n",
       "117  https://www.politifact.com/personalities/insta...  July 14, 2023   \n",
       "118  https://www.politifact.com/personalities/insta...  July 14, 2023   \n",
       "119  https://www.politifact.com/personalities/insta...  July 14, 2023   \n",
       "\n",
       "              Source  Label  \n",
       "0           Bloggers  false  \n",
       "1     Facebook posts  false  \n",
       "2     Facebook posts  false  \n",
       "3    Instagram posts  false  \n",
       "4       Charlie Kirk  false  \n",
       "..               ...    ...  \n",
       "115   Facebook posts  false  \n",
       "116     TikTok posts  false  \n",
       "117  Instagram posts  false  \n",
       "118  Instagram posts  false  \n",
       "119  Instagram posts  false  \n",
       "\n",
       "[120 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<article class=\"sc-1pw4fyi-6 hGCNFC sc-1oawalh-4 dPimIa js_post_item\" data-id=\"1850725546\"><div class=\"sc-1pw4fyi-4 ljPZBC\"><a aria-label=\"Link to DeSantis Has Surprisingly Smooth Verbal Exchange With Iowa State Fair Corn Dog\" class=\"sc-1out364-0 dPMosf sc-1pw4fyi-5 hQxRDX js_link\" data-ga='[[\"Home page click\",\"Modular Curation Story Click - slot 1 - story 2 of 8\",\"https://www.theonion.com/desantis-has-surprisingly-smooth-verbal-exchange-with-i-1850725546\"]]' href=\"https://www.theonion.com/desantis-has-surprisingly-smooth-verbal-exchange-with-i-1850725546\" target=\"_self\" title=\"DeSantis Has Surprisingly Smooth Verbal Exchange With Iowa State Fair Corn Dog\"><div class=\"sc-1dm5z0l-1 hPYqxf\"><div class=\"sc-1dm5z0l-0 ewYewO\"><picture class=\"sc-epkw7d-0 bHizmP\"><source media=\"(max-width: 37.31em)\" srcset=\"https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,g_center,h_362,q_60,w_645/702e9c19f7b1c45f4807a4c53c11db93.jpg\" type=\"image/jpeg\"/><source media=\"(min-width: 37.37em) and (max-width: 49.94em)\" srcset=\"https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,g_center,h_416,q_60,w_740/702e9c19f7b1c45f4807a4c53c11db93.jpg\" type=\"image/jpeg\"/><source media=\"(min-width: 50em)\" srcset=\"https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,g_center,h_191,q_60,w_340/702e9c19f7b1c45f4807a4c53c11db93.jpg\" type=\"image/jpeg\"/><img alt=\"Image for DeSantis Has Surprisingly Smooth Verbal Exchange With Iowa State Fair Corn Dog\" data-alt=\"Image for DeSantis Has Surprisingly Smooth Verbal Exchange With Iowa State Fair Corn Dog\" data-anim-src=\"\" data-chomp-id=\"702e9c19f7b1c45f4807a4c53c11db93\" data-format=\"jpg\" src=\"https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,g_center,h_416,q_60,w_740/702e9c19f7b1c45f4807a4c53c11db93.jpg\"/></picture></div></div></a></div><div class=\"sc-1pw4fyi-3 fxRxEE\"><div class=\"sc-1pw4fyi-7 fqdmzv\"><div class=\"sc-1hjwdsc-2\"><a class=\"sc-1out364-0 dPMosf sc-1hjwdsc-0 iKwYcT js_link\" color=\"#006B3A\" data-ga='[[\"Home page click\",\"Modular Curation Story Type Click - slot 1 - story 2 of 8\",\"https://www.theonion.com/desantis-has-surprisingly-smooth-verbal-exchange-with-i-1850725546\"]]' href=\"https://www.theonion.com/politics\">Politics</a></div><a class=\"sc-1out364-0 dPMosf sc-1pw4fyi-5 hQxRDX js_link\" data-ga='[[\"Home page click\",\"Modular Curation Story Click - slot 1 - story 2 of 8\",\"https://www.theonion.com/desantis-has-surprisingly-smooth-verbal-exchange-with-i-1850725546\"]]' href=\"https://www.theonion.com/desantis-has-surprisingly-smooth-verbal-exchange-with-i-1850725546\" target=\"_self\"><h4 class=\"sc-1qoge05-0 jqRtko\">DeSantis Has Surprisingly Smooth Verbal Exchange With Iowa State Fair Corn Dog</h4></a><div class=\"sc-ysh9pk-0 hYetNt\"></div></div></div></article>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page: 1\n",
      "Error parsing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.politifact.com/article/2018/feb/12/principles-truth-o-meter-politifacts-methodology-i/%20 on URL https://www.politifact.com/article/2018/feb/12/principles-truth-o-meter-politifacts-methodology-i/ \n",
      "Processing page: 2\n",
      "Error parsing article: Article `download()` failed with 404 Client Error: Not Found for url: https://www.politifact.com/article/2018/feb/12/principles-truth-o-meter-politifacts-methodology-i/%20 on URL https://www.politifact.com/article/2018/feb/12/principles-truth-o-meter-politifacts-methodology-i/ \n",
      "Processing page: 3\n",
      "Processing page: 4\n",
      "                                           Statement  \\\n",
      "0  Political Extremism in the Public Square: A re...   \n",
      "1  Fabricated story about Vladimir Putin banning ...   \n",
      "2  Are black eyes on newsmakers evidence they’re ...   \n",
      "3  Story about Jason Aldean suing Whoopi Goldberg...   \n",
      "4  World Economic Forum’s Klaus Schwab didn’t cal...   \n",
      "\n",
      "                                                Link        Date      Source  \\\n",
      "0                 https://www.politifact.com/vermont  2022-06-09  Politifact   \n",
      "1  https://www.politifact.com/factchecks/list/?pa...  2023-08-10  Politifact   \n",
      "2  https://www.politifact.com/factchecks/list/?pa...  2023-08-10  Politifact   \n",
      "3  https://www.politifact.com/factchecks/list/?pa...  2023-08-10  Politifact   \n",
      "4  https://www.politifact.com/factchecks/list/?pa...  2023-08-09  Politifact   \n",
      "\n",
      "   Label  \n",
      "0  Label  \n",
      "1  Label  \n",
      "2  Label  \n",
      "3  Label  \n",
      "4  Label  \n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "\n",
    "# Set the number of articles to scrape\n",
    "articles_to_scrape = 200\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through pages until the desired number of articles is reached\n",
    "for page in range(1, (articles_to_scrape // 50) + 1):\n",
    "    print('Processing page:', page)\n",
    "    \n",
    "    # Construct the URL for the page\n",
    "    url = f'https://www.politifact.com/factchecks/list/?page={page}'\n",
    "    \n",
    "    # Use the newspaper library to extract article information\n",
    "    paper = newspaper.build(url, memoize_articles=False)\n",
    "    \n",
    "    for article in paper.articles:\n",
    "        if len(data) >= articles_to_scrape:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Download and parse the article\n",
    "            article.download()\n",
    "            article.parse()\n",
    "        except Exception as e:\n",
    "            print('Error parsing article:', e)\n",
    "            continue\n",
    "        \n",
    "        # Extract information from the article\n",
    "        statement = article.title\n",
    "        link = article.source_url\n",
    "        date = article.publish_date.strftime('%Y-%m-%d') if article.publish_date else ''\n",
    "        source = 'Politifact'\n",
    "        label = 'Label'  # Add a method to extract the label if available\n",
    "        \n",
    "        # Append the article information to the data list\n",
    "        data.append((statement, link, date, source, label))\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data, columns=['Statement', 'Link', 'Date', 'Source', 'Label'])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m combined_data \u001b[39m=\u001b[39m df_real\u001b[39m.\u001b[39mmerge(df_fake)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_real' is not defined"
     ]
    }
   ],
   "source": [
    "combined_data = df_real.merge(df_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
